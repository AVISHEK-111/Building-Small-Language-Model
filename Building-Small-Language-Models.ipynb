{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:17:05.973357Z","iopub.execute_input":"2026-02-07T10:17:05.973946Z","iopub.status.idle":"2026-02-07T10:17:09.960237Z","shell.execute_reply.started":"2026-02-07T10:17:05.973918Z","shell.execute_reply":"2026-02-07T10:17:09.959449Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install -U datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:17:09.961842Z","iopub.execute_input":"2026-02-07T10:17:09.962248Z","iopub.status.idle":"2026-02-07T10:17:13.974698Z","shell.execute_reply.started":"2026-02-07T10:17:09.962218Z","shell.execute_reply":"2026-02-07T10:17:13.973766Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\nCollecting datasets\n  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading datasets-4.5.0-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.2\n    Uninstalling datasets-4.4.2:\n      Successfully uninstalled datasets-4.4.2\nSuccessfully installed datasets-4.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:17:13.976091Z","iopub.execute_input":"2026-02-07T10:17:13.976360Z","iopub.status.idle":"2026-02-07T10:17:18.588108Z","shell.execute_reply.started":"2026-02-07T10:17:13.976331Z","shell.execute_reply":"2026-02-07T10:17:18.587182Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pip-26.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\nds = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:17:18.590353Z","iopub.execute_input":"2026-02-07T10:17:18.590614Z","iopub.status.idle":"2026-02-07T10:17:37.779811Z","shell.execute_reply.started":"2026-02-07T10:17:18.590587Z","shell.execute_reply":"2026-02-07T10:17:37.779233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a8cca9ed30e46e4b5edaf5f004f5df2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00004-2d5a1467fff108(…):   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cb9a246487b4a31a6e12495d4f8a4bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00004-5852b56a2bd28f(…):   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42598d8719749f8b41f67044d6b1b16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00002-of-00004-a26307300439e9(…):   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb249e5b52a24275bac151c6661fdc41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00003-of-00004-d243063613e5a0(…):   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceaf7dcefff946e39af5b6d3dc089d98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-869c898b5(…):   0%|          | 0.00/9.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"498ee4211f154fd7a8413f0ecd27f7e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f039a2b4b404d239f2360ea436dc553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f41ddbb757426585fee063e6333e03"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"!pip install tiktoken\nimport tiktoken\nimport os\nimport numpy as np\nfrom tqdm.auto import tqdm   # progress bars\n\nenc = tiktoken.get_encoding(\"gpt2\")  # it is byte pair encoding\n\n# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n\ndef process(example):\n    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n    out = {'ids': ids, 'len': len(ids)}\n    return out\n\nif not os.path.exists(\"train.bin\"):\n    tokenized = ds.map(\n        process,\n        remove_columns=['text'],\n        desc=\"tokenizing the splits\",\n        num_proc=8,\n        )\n    # concatenate all the ids in each dataset into one large file we can use for training\n    for split, dset in tokenized.items():\n        arr_len = np.sum(dset['len'], dtype=np.uint64)\n        filename = f'{split}.bin'\n        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n        total_batches = 1024\n\n        idx = 0\n        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n            # Batch together samples for faster write\n            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n            arr_batch = np.concatenate(batch['ids']) # flattening the sequences in batch to one array\n            # Write into mmap\n            arr[idx : idx + len(arr_batch)] = arr_batch # write batch to a memory mapped file\n            idx += len(arr_batch)\n        arr.flush() # this ensures all data is written to the disk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:17:37.780584Z","iopub.execute_input":"2026-02-07T10:17:37.781038Z","iopub.status.idle":"2026-02-07T10:34:32.567704Z","shell.execute_reply.started":"2026-02-07T10:17:37.781012Z","shell.execute_reply":"2026-02-07T10:34:32.566857Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizing the splits (num_proc=8):   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ab87e54936429a9ef343b98c42fa6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizing the splits (num_proc=8):   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15527321b004af497c9aa82fffda6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"writing train.bin:   0%|          | 0/1024 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5efb7bcc49af46019f8992357c9ca23a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"writing validation.bin:   0%|          | 0/1024 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6022bcd0277642edba95e40a7946c13a"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import torch\ndef get_batch(split):\n  # We create np.memmap every batch to avoid a memory leak\n  if split =='train':\n    data = np.memmap('train.bin', dtype = np.uint16, mode = 'r')\n  else:\n    data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n\n  ix = torch.randint(len(data)-block_size, (batch_size,))\n  x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n  y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n  if device_type =='cuda':\n    #pin arrays x,y, which allows to move them to GPU asynchronously\n    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n  else:\n    x, y = x.to(device), y.to(device)\n  return x,y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:34:32.568947Z","iopub.execute_input":"2026-02-07T10:34:32.569230Z","iopub.status.idle":"2026-02-07T10:34:32.575790Z","shell.execute_reply.started":"2026-02-07T10:34:32.569204Z","shell.execute_reply":"2026-02-07T10:34:32.575160Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom dataclasses import dataclass\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom contextlib import nullcontext\nimport os\n\nclass LayerNorm(nn.Module):\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n    def forward(self, x):\n        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.flash = hasattr(F, 'scaled_dot_product_attention')\n        if not self.flash:\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                       .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n        if self.flash:\n            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = LayerNorm(config.n_embd, config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = LayerNorm(config.n_embd, config.bias)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int\n    vocab_size: int\n    n_layer: int\n    n_head: int\n    n_embd: int\n    dropout: float = 0.0\n    bias: bool = True\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            wpe=nn.Embedding(config.block_size, config.n_embd),\n            drop=nn.Dropout(config.dropout),\n            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f=LayerNorm(config.n_embd, config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n            return logits, loss\n        else:\n            logits = self.lm_head(x[:, [-1], :])\n            return logits, None\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Generate tokens given a conditioning sequence.\n        idx: Tensor of shape (B, T)\n        \"\"\"\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:34:32.576628Z","iopub.execute_input":"2026-02-07T10:34:32.576906Z","iopub.status.idle":"2026-02-07T10:34:32.601908Z","shell.execute_reply.started":"2026-02-07T10:34:32.576884Z","shell.execute_reply":"2026-02-07T10:34:32.601307Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"config = GPTConfig(\n    vocab_size=50257,     # use the tokenizer's vocab size\n    block_size=128,       # or whatever context size you're training with\n    n_layer=6,\n    n_head=6,\n    n_embd=384,\n    dropout=0.1,\n    bias=True\n)\n\nmodel = GPT(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:34:32.602791Z","iopub.execute_input":"2026-02-07T10:34:32.603046Z","iopub.status.idle":"2026-02-07T10:34:33.310370Z","shell.execute_reply.started":"2026-02-07T10:34:32.603018Z","shell.execute_reply":"2026-02-07T10:34:33.309752Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def estimate_loss(model):\n    out = {}\n    model.eval()\n    with torch.inference_mode():\n        for split in ['train', 'val']:\n            losses = torch.zeros(eval_iters)\n            for k in range(eval_iters):\n                X, Y = get_batch(split)\n                with ctx:\n                    logits, loss = model(X, Y)\n                losses[k] = loss.item()\n            out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:34:33.311431Z","iopub.execute_input":"2026-02-07T10:34:33.312046Z","iopub.status.idle":"2026-02-07T10:34:33.316697Z","shell.execute_reply.started":"2026-02-07T10:34:33.312013Z","shell.execute_reply":"2026-02-07T10:34:33.316025Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Training Config\nimport torch\nfrom contextlib import nullcontext\n\nlearning_rate = 1e-4 #more stable training, earlier 1e-4\nmax_iters = 20000 #increase from 25000\nwarmup_steps = 1000 #smoother initial train, earlier 100\nmin_lr = 5e-4 #lower rate, earlier 5e-4\neval_iters = 500 # increased from 100\nbatch_size = 32 # changed from 16, better gradient estimate\nblock_size = 128 #changed from 64, capture longer range dependencies\n\ngradient_accumulation_steps = 32 # reduced from 50\n\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\n\n# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\ntorch.set_default_device(device)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:34:33.318845Z","iopub.execute_input":"2026-02-07T10:34:33.319219Z","iopub.status.idle":"2026-02-07T10:34:33.524859Z","shell.execute_reply.started":"2026-02-07T10:34:33.319197Z","shell.execute_reply":"2026-02-07T10:34:33.524125Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7e6b4abf03f0>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n\n##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\noptimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n\nscheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\nscheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\nscheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n\n# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:34:33.525905Z","iopub.execute_input":"2026-02-07T10:34:33.526598Z","iopub.status.idle":"2026-02-07T10:34:36.615782Z","shell.execute_reply.started":"2026-02-07T10:34:33.526574Z","shell.execute_reply":"2026-02-07T10:34:36.615013Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/2132813893.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"best_val_loss = float('inf')\nbest_model_params_path = \"best_model_params.pt\"\ntrain_loss_list, validation_loss_list = [], []\n\n# Ensure model is on the correct device\nmodel = model.to(device)\n\n# In your training loop\nfor epoch in tqdm(range(max_iters)):\n    if epoch % eval_iters == 0 and epoch != 0:\n        # Ensure estimate_loss uses the correct device\n        losses = estimate_loss(model)\n        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n        train_loss_list += [losses['train']]\n        validation_loss_list += [losses['val']]\n\n        if losses['val'] < best_val_loss:\n            best_val_loss = losses['val']\n            torch.save(model.state_dict(), best_model_params_path)\n\n    # Ensure X and y are on the correct device\n    X, y = get_batch(\"train\")\n    X, y = X.to(device), y.to(device)\n\n    with ctx:\n        logits, loss = model(X, y)\n        loss = loss / gradient_accumulation_steps\n        scaler.scale(loss).backward()\n\n    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:34:36.616871Z","iopub.execute_input":"2026-02-07T10:34:36.617320Z","iopub.status.idle":"2026-02-07T12:13:14.500795Z","shell.execute_reply.started":"2026-02-07T10:34:36.617296Z","shell.execute_reply":"2026-02-07T12:13:14.499936Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed916985484744b191360431cd67800d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 500: train loss 9.4171, val loss 9.4240\nThe current learning rate: 0.00007\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1000: train loss 8.4629, val loss 8.4670\nThe current learning rate: 0.00010\nEpoch 1500: train loss 7.4926, val loss 7.4913\nThe current learning rate: 0.00010\nEpoch 2000: train loss 6.6445, val loss 6.6484\nThe current learning rate: 0.00010\nEpoch 2500: train loss 5.9576, val loss 5.9560\nThe current learning rate: 0.00011\nEpoch 3000: train loss 5.4578, val loss 5.4524\nThe current learning rate: 0.00011\nEpoch 3500: train loss 5.0443, val loss 5.0467\nThe current learning rate: 0.00012\nEpoch 4000: train loss 4.7332, val loss 4.7305\nThe current learning rate: 0.00012\nEpoch 4500: train loss 4.4929, val loss 4.4962\nThe current learning rate: 0.00013\nEpoch 5000: train loss 4.2872, val loss 4.2781\nThe current learning rate: 0.00014\nEpoch 5500: train loss 4.1089, val loss 4.1095\nThe current learning rate: 0.00015\nEpoch 6000: train loss 3.9608, val loss 3.9647\nThe current learning rate: 0.00016\nEpoch 6500: train loss 3.8283, val loss 3.8280\nThe current learning rate: 0.00018\nEpoch 7000: train loss 3.7083, val loss 3.7221\nThe current learning rate: 0.00019\nEpoch 7500: train loss 3.6015, val loss 3.6053\nThe current learning rate: 0.00020\nEpoch 8000: train loss 3.4875, val loss 3.4926\nThe current learning rate: 0.00022\nEpoch 8500: train loss 3.4011, val loss 3.4050\nThe current learning rate: 0.00024\nEpoch 9000: train loss 3.3171, val loss 3.3228\nThe current learning rate: 0.00025\nEpoch 9500: train loss 3.2397, val loss 3.2407\nThe current learning rate: 0.00027\nEpoch 10000: train loss 3.1688, val loss 3.1799\nThe current learning rate: 0.00028\nEpoch 10500: train loss 3.1007, val loss 3.1114\nThe current learning rate: 0.00030\nEpoch 11000: train loss 3.0467, val loss 3.0532\nThe current learning rate: 0.00032\nEpoch 11500: train loss 2.9857, val loss 2.9857\nThe current learning rate: 0.00033\nEpoch 12000: train loss 2.9323, val loss 2.9413\nThe current learning rate: 0.00035\nEpoch 12500: train loss 2.8849, val loss 2.8828\nThe current learning rate: 0.00036\nEpoch 13000: train loss 2.8325, val loss 2.8382\nThe current learning rate: 0.00038\nEpoch 13500: train loss 2.7884, val loss 2.7969\nThe current learning rate: 0.00040\nEpoch 14000: train loss 2.7483, val loss 2.7540\nThe current learning rate: 0.00041\nEpoch 14500: train loss 2.7119, val loss 2.7115\nThe current learning rate: 0.00042\nEpoch 15000: train loss 2.6621, val loss 2.6674\nThe current learning rate: 0.00044\nEpoch 15500: train loss 2.6281, val loss 2.6267\nThe current learning rate: 0.00045\nEpoch 16000: train loss 2.5929, val loss 2.5940\nThe current learning rate: 0.00046\nEpoch 16500: train loss 2.5470, val loss 2.5549\nThe current learning rate: 0.00047\nEpoch 17000: train loss 2.5215, val loss 2.5304\nThe current learning rate: 0.00048\nEpoch 17500: train loss 2.5008, val loss 2.5011\nThe current learning rate: 0.00048\nEpoch 18000: train loss 2.4605, val loss 2.4736\nThe current learning rate: 0.00049\nEpoch 18500: train loss 2.4355, val loss 2.4435\nThe current learning rate: 0.00049\nEpoch 19000: train loss 2.4138, val loss 2.4180\nThe current learning rate: 0.00050\nEpoch 19500: train loss 2.3910, val loss 2.3965\nThe current learning rate: 0.00050\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntrain_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\nvalidation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n\nplt.plot(train_loss_list_converted, 'g', label='train_loss')\nplt.plot(validation_loss_list_converted, 'r', label='validation_loss')\nplt.xlabel(\"Steps - Every 100 epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:13:14.501938Z","iopub.execute_input":"2026-02-07T12:13:14.502557Z","iopub.status.idle":"2026-02-07T12:13:14.681679Z","shell.execute_reply.started":"2026-02-07T12:13:14.502532Z","shell.execute_reply":"2026-02-07T12:13:14.681041Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"#Load the model\nmodel = GPT(config)  # re-create the model with same config\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbest_model_params_path = \"best_model_params.pt\"\nmodel.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:13:14.682591Z","iopub.execute_input":"2026-02-07T12:13:14.682846Z","iopub.status.idle":"2026-02-07T12:13:14.823927Z","shell.execute_reply.started":"2026-02-07T12:13:14.682824Z","shell.execute_reply":"2026-02-07T12:13:14.823126Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"sentence = \"Once upon a time there was a pumpkin.\"\ncontext = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\ny = model.generate(context, 200)\nprint(enc.decode(y.squeeze().tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:13:14.825126Z","iopub.execute_input":"2026-02-07T12:13:14.825383Z","iopub.status.idle":"2026-02-07T12:13:15.912565Z","shell.execute_reply.started":"2026-02-07T12:13:14.825359Z","shell.execute_reply":"2026-02-07T12:13:15.911893Z"}},"outputs":[{"name":"stdout","text":"Once upon a time there was a pumpkin. You are in your home. I am cold.\"\n\nThe triangle didn't want a treat. It made a loud yell.\n\nThe pumpkin catches a gorilla. What are together that bounce at me? We have to sleep.\"\n\nThey both give the nightmares some sandwiches and shirt. When the shark was dancing.\n\n\"What do you want?\" perched said the squash. \"We lost it,\" he said.\n\n\"Feufumber is hurt of our boo! I love you not!\"\n\nThe cricket licked theirhungy said, \"It is normal. You need to wear it. You need to go home so he should have gotten your food now. And you have to be gentle and share them to wait or wash your pieces. You are a good friend.\"\n\nThe crab smiles. He hugs Lisa and licked her. He shows them each to Lily. He is the duck faster and flaps his neck.\n\n\"Here, let coming\n","output_type":"stream"}],"execution_count":19}]}